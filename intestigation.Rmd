---
title: "IDS investigation worksheet"
author: "by Binary_Brigade: Connor Finlay, Ade Obawole, Murray Bone, Alexander Patton & Cameron Smith"
date: "`r Sys.Date()`"
output: html_document
---

**Note:** You can use this file as you 'working document' where you can try out various investigation ideas and keep notes about your findings. How you use and structure this file is up to you. It is recommended that you keep notes about what you are investigating and what you find as this will make the process of creating your presentation and report easier. Please note that you *do not* need to submit this file as part of your group project.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-lib, message = FALSE}
library(tidyverse)
library(cowplot)
library(lubridate)
library(tidymodels)
```

```{r load-data}
# adding a and e data

aANDe <- read_csv("data/a5f7ca94-c810-41b5-a7c9-25c18d43e5a4.csv")

# adding covid data
covid_nottidy <- read_csv("data/b5e3fa11-8a85-4946-bbb2-2e800d4e3594.csv")

nhs_workforce_nottidy <- read_csv("data/data.csv") # Reads data collected from the NHS Scotland's Turas system about the total amount of vacancies across NHS Scotland's allied health professions. Latest release as of Nov 13 2023 https://turasdata.nes.nhs.scot/data-and-reports/official-workforce-statistics/all-official-statistics-publications/05-september-2023-workforce/dashboards/nhs-scotland-workforce/?pageid=9984

```

sorting Date and Time

```{r data-tidying}
unique(aANDe$DepartmentType)
unique(aANDe$Country)

 AandE <- aANDe %>% 
  mutate(
    weekend_date_correct = ymd(WeekEndingDate),
    quarter = quarter(weekend_date_correct, with_year = TRUE),
    ) %>% # Gets the date information for each observation into the datetime and quarter format
     select(-c(WeekEndingDate, DepartmentType, Country)
         ) # Removes irrelevant (superceded/the same for all observations) from the data set
 
 Covid <- covid_nottidy %>% 
   mutate(
     weekend_date_correct = ymd(Date)
   ) %>% 
   select(-c(Date))

nhs_workforce <- nhs_workforce_nottidy %>%
  mutate(
    census_date = ymd(`Census date`),
    quarter = quarter(census_date, with_year = TRUE),
    vacancies = `Value (WTE)`
  ) %>%
      select(
        -c(`Census date`,`Value (WTE)`,...3,`Vacancy note`)
      )
  
```

Summarising wait time over 4 hours based on location of treatment and percentage within 4 hours by year

```{r spare}
AandE %>% 
  group_by(TreatmentLocation) %>% 
  summarise(
    avg_pct_Within4 = mean(PercentageWithin4HoursEpisode),
    avg_pct_Over8 = mean(PercentageOver8HoursEpisode),
    avg_pct_Over12 = mean(PercentageOver12HoursEpisode)
    )
#It is evident that based on the location of the practice, the average amount of people that wait 4/8/12 hours vary drastically. Summarising this allows us to witness what the average amount of wait time is based on individual practice, over all weeks.
AandE %>% 
  mutate(year = year(weekend_date_correct)) %>%
  group_by(year) %>% 
  summarise( 
    med_pct_Within4 = median(PercentageWithin4HoursEpisode) ) %>% 
  ggplot( mapping = aes( x = year,
                         y = med_pct_Within4 
                         ) 
          ) + 
  geom_bar(stat='identity') + 
  theme_bw() # The median percentage seen to within 4 hours seems to decrease over time (especially after 2020) - Why?

AandE %>%
  mutate(year = year(weekend_date_correct)) %>%
  group_by(year) %>% 
  ggplot(
    mapping = aes(
      x = year
    )
    ) +
    geom_bar() +
      theme_bw() # We can see that there is an equivalent amount of observations for each complete year (2016-2022) in this data set; this is because the data was collected into weekly sets for roughly same group of A&Es, meaning that the number of observations was unlikely to change 
    
    
AandE %>%
  group_by(weekend_date_correct) %>% 
  summarise(
    med_pct_Within4 = median(PercentageWithin4HoursEpisode)
  ) %>%
  ggplot(
    mapping = aes(
      x = weekend_date_correct,
      y = med_pct_Within4
    ) 
  ) +
geom_freqpoly(stat="identity") +
  theme_bw() # Gives a more granular view of the trend seen before; the median percentage of attendances seen within 4 hours decreases particularly after 2020. It consistently has much lower values in 2022 and 2023, bottoming out around the beginning of 2023. 

AandE %>%
 group_by(weekend_date_correct) %>% 
  summarise(
    sum_Within4 = sum(NumberWithin4HoursEpisode)
  ) %>%
  ggplot(
    mapping = aes(
      x = weekend_date_correct,
      y = sum_Within4
    ) 
  ) +
geom_freqpoly(stat="identity") +
  theme_bw() # Shows a  more granular tabulation of waits under 4 hours, giving on a weekly basis. This allows us to see that the total number of visits under 4 hours has also decreased since the beginning of the data set, bottoming out in early-to-mid 2020 and staying chaotic but generally low until today. 

AandE %>%
  group_by(weekend_date_correct) %>% 
  filter(year(weekend_date_correct) <= 2016
  ) %>%
  summarise(
    med_pct_Within4 = median(PercentageWithin4HoursEpisode)
  ) %>%
  ggplot(
    mapping = aes(
      x = weekend_date_correct,
      y = med_pct_Within4
    ) 
  ) +
geom_freqpoly(stat="identity") +
  theme_bw() # Creates a version of the first frequency plot limited to 2015 and 2016 to see if the closure of certain A&Es during 2015 had an effect that would be more noticeable at this scale. Around that time, no significant decrease appears to occur.

AandE %>%
  group_by(weekend_date_correct) %>% 
  filter(year(weekend_date_correct) >= 2022
  ) %>%
  summarise(
    med_pct_Within4 = median(PercentageWithin4HoursEpisode)
  ) %>%
  ggplot(
    mapping = aes(
      x = weekend_date_correct,
      y = med_pct_Within4
    ) 
  ) +
geom_freqpoly(stat="identity") +
  theme_bw() # Creates a version of the first frequency plot limited to 2022 and 2023 to get a closer look at the minimum at the start of 2023. The surrounding weeks appear to have similarly low values, signifying that this especially low amount of attendances under 4 hours may have been part of some greater trend. 
  

  


```

```{r}

Covidgraph <- AandE %>% filter(weekend_date_correct > ym("2020-03"), weekend_date_correct <= ym("2023-09")) 

p1 <- ggplot(Covidgraph, aes(x = weekend_date_correct,
                        y = NumberOfAttendancesEpisode)) +
  geom_smooth() + labs( y = "number of AandE admissions",
                       x = "date",
                        title = "A and E Admissions by data during covid pandemic")

p2 <- ggplot(Covid, aes(x = weekend_date_correct,
                  y = NumberAdmitted)) +
  geom_smooth() + labs(x = "date",
                       y = "number of covid hospitalisations",
                       title = "number of covid admissions")

combined_plot <- plot_grid(p1, p2)

print(combined_plot)
```

From this it can be seen that the number of A and E admissions is directly inproportionate to the number of covid admissions.

```{r}

AandE$Z_Score_Within4Hours <- scale(AandE$PercentageWithin4HoursEpisode)
outlier_threshold <- 3
outliers_within4hours <- AandE[abs(AandE$Z_Score_Within4Hours) > outlier_threshold, ]
print(outliers_within4hours)



```

This analysis calculates Z-scores for 'PercentageWithin4HoursEpisode,' helping us spot outliers which are data points that are significantly different from the average waiting time. I've set a threshold at 3 standard deviations meaning that if a week's Z-score exceeds 3 or falls below -3, it's flagged as an outlier in the 'outliers_within4hours' data frame. This helps us focus on weeks with unusual waiting time patterns.
```{r}
p3 <- AandE %>% 
  mutate(year = year(weekend_date_correct)) %>%
  group_by(quarter) %>%
  summarise( 
    med_pct_Within4 = median(PercentageWithin4HoursEpisode) ) %>% 
  ggplot( mapping = aes( 
    x = quarter,
    y = med_pct_Within4 
                         ) 
          ) + 
  geom_freqpoly(stat='identity') + 
  geom_smooth() +
  theme_bw() # Creates a plot of median percentage of waiting times for quarter of data collection to compare against a similar plot of vacancies

p4 <- nhs_workforce %>%
  ggplot(
    mapping = aes(
      x = quarter,
      y = vacancies
    )
  ) +
  geom_freqpoly(stat="identity") +
  geom_smooth() +
  theme_bw() # Creates a plot of median percentage of vacancies among allied health NHS Scotland workers for every quarter of data collection to compare against the above plot

  plot_grid(p3,p4) # Creates a plot grid of the last two plots to more easily compare their shapes and trends
  
  AandE_work <- AandE %>%
    inner_join(
      by = "quarter",
      nhs_workforce
    ) %>%
    group_by(quarter) %>%
      summarise(
        med_pct_within4 = median(PercentageWithin4HoursEpisode),
        vacancies = median(vacancies)
      )
 p5 <- AandE_work %>%
    ggplot(
      mapping = aes(
        x = vacancies,
        y = med_pct_within4,
        colour = quarter
      )
    ) +
    geom_point() +
    geom_smooth(method='lm') +
    theme_bw()
  
  p5 # Plots the median percentage of visits within 4 hours for a certain quarter against the vacancies recorded for that quarter and fits a preliminary linear model to it. DO A PROPER LINEA$R REGRESSION
  
```

From the above preliminary data visualisations,  it seems that the portion of A&E visits seen to in 4 hours, in Scotland for a given quarter, can be inversely related to the amount of vacancies (Whole Time Equivalent) wihtin NHS Scotland. 

```{r}
AEW_fit <- linear_reg() %>% # Initiates a linear regression
  set_engine("lm") %>% # Gives this linear regression a linear model engine
    fit(med_pct_within4 ~ vacancies, 
        data = AandE_work,
        family = "binomial"
    )
tidy(AEW_fit)
glance(AEW_fit)$r.squared

```

This constructs a preliminary linear model that shows a inverse relation between the median percentage of visits within an hour and the total number of vacancies in the health system. The R-squared for this is over 0.9, meaning that over 90% of the variance in med_pct_within4 is explained by it. However, the intercept reflects a percentage over 100%, which might be an issue.  